{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text generation using a Recurrent Neural Network (LSTM).\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "from tensorflow.python.framework import graph_util # used for exporting the graph\n",
    "\n",
    "class ModelNetwork:\n",
    "    \"\"\"\n",
    "    RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "    The network allows for a dynamic number of iterations, depending on the\n",
    "    inputs it receives.\n",
    "\n",
    "       out   (fc layer; out_size)\n",
    "        ^\n",
    "       lstm\n",
    "        ^\n",
    "       lstm  (lstm size)\n",
    "        ^\n",
    "        in   (in_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size,output_tensor, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.output_tensor = output_tensor\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            \n",
    "            #network_output_1 = f.matmul(outputs_reshaped,self.rnn_out_W  )+self.rnn_out_B\n",
    "            \n",
    "            network_output_1 = tf.matmul(outputs_reshaped,self.rnn_out_W  )\n",
    "            network_output = tf.add(network_output_1, self.rnn_out_B,  name =\"outputXYZ\")  \n",
    "            \n",
    "#             network_output_1 =  tf.add(network_output,  name =self.output_tensor)                      \n",
    "                                      \n",
    "            \n",
    "            #network_output = tf.identity(network_output_1, self.output_tensor)\n",
    "            '''network_output = tf.add(tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ),self.rnn_out_B,name = output_tensor)'''\n",
    "                \n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size),       \n",
    "            )\n",
    "            #,name = output_tensor\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(input):\n",
    "    # Load the data\n",
    "    data_ = \"\"\n",
    "    with open(input, 'r') as f:\n",
    "        data_ += f.read()\n",
    "    data_ = data_.lower()\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below cell\n",
    "#args = parser.parse_args()\n",
    "mode=\"train\"\n",
    "\n",
    "TEST_PREFIX = \"The \" # Prefix to prompt the network in test mode\n",
    "\n",
    "#if args.ckpt_file:\n",
    "    #ckpt_file = args.ckpt_file\n",
    "\n",
    "# Load the data\n",
    "#data, vocab = load_data('tokenized_part_instructions_train.txt')\n",
    "data, vocab = load_data('dataset/tokenized_recipe_train1.txt')\n",
    "#data, vocab = load_data('shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "save_path = \"./model/\"\n",
    "in_size = out_size = len(vocab)\n",
    "lstm_size = 256  # 128\n",
    "num_layers = 2\n",
    "batch_size = 32 #64  # 128\n",
    "time_steps = 500  # 50\n",
    "\n",
    "NUM_TRAIN_BATCHES = 10000\n",
    "\n",
    "# Number of test characters of text to generate after training the network\n",
    "LEN_TEST_TEXT = 300\n",
    "\n",
    "\n",
    "output_tensor = \"output\"\n",
    "# Initialize the network\n",
    "GPU_USE = '/gpu:0'\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement=True\n",
    "config.allow_soft_placement=True\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.6\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "scope = \"char_rnn_network\"\n",
    "with tf.Session(config=config) as sess:\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        output_tensor=output_tensor,\n",
    "        session=sess,\n",
    "        learning_rate=0.001,\n",
    "        name=scope\n",
    "    )\n",
    "\n",
    "    #print(net.output_tensor)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # 1) TRAIN THE NETWORK\n",
    "    if mode == \"train\":\n",
    "        check_restore_parameters(sess, saver)\n",
    "        last_time = time.time()\n",
    "        batch = np.zeros((batch_size, time_steps, in_size))\n",
    "        batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "        possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "        for i in range(NUM_TRAIN_BATCHES):\n",
    "            # Sample time_steps consecutive samples from the dataset text file\n",
    "            batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "            for j in range(time_steps):\n",
    "                ind1 = [k + j for k in batch_id]\n",
    "                ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "                batch[:, j, :] = data[ind1, :]\n",
    "                batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "            cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "            if (i % 100) == 0:\n",
    "                new_time = time.time()\n",
    "                diff = new_time - last_time\n",
    "                last_time = new_time\n",
    "                print(\"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                    i, cst, 100 / diff\n",
    "                ))\n",
    "\n",
    "        #saver.save(sess, save_path + \"model\", write_meta_graph=True)\n",
    "        #tf.train.write_graph(sess.graph_def, '.', './saved/model_recipe.pbtxt')  \n",
    "        saver.save(sess, save_path + \"model_recipe\", write_meta_graph=True)\n",
    "        #saver.save(sess, save_path + \"model\", global_step=epoch, write_meta_graph=True)\n",
    "        #saver.save(sess, save_path + \"model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#output_tensor = \"final_outputs\"\n",
    "#output_tensor = \"network_output\"\n",
    "#output_tensor = \"outputs_reshaped\"\n",
    "#output_tensor = \"outputs\"\n",
    "output_tensor = scope+\"/outputXYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_folder, model_file_name = None):\n",
    "    GPU_USE = '/cpu:0'\n",
    "    config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "    \n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    print (absolute_model_folder)\n",
    "    if (model_file_name == None):\n",
    "        model_file_name = input_checkpoint.split(\"/\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        input_checkpoint = absolute_model_folder + \"/\" + model_file_name\n",
    "    print (model_file_name)\n",
    "    \n",
    "    # If you use our Script outside of the tutorial please specify your path to store the model\n",
    "    output_graph = absolute_model_folder + \"/rnnmodel.pb\"\n",
    "\n",
    "    # Output node for TensorFlow to decide which part of the Graph to keep.\n",
    "\n",
    "    print(\"Saver is loading: \" + input_checkpoint + '.meta')\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\n",
    "\n",
    "    # We retrieve the protobuf graph definition\n",
    "    graph = tf.get_default_graph()\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "\n",
    "    # start session and write the exported file. \n",
    "    with tf.device(GPU_USE):\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, input_checkpoint)\n",
    "\n",
    "            output_graph_def = graph_util.convert_variables_to_constants(\n",
    "                sess, # The session is used to retrieve the weights\n",
    "                input_graph_def, # The graph_def is used to retrieve the nodes \n",
    "                [output_tensor] # The output node names are used to select the useful nodes.\n",
    "            ) \n",
    "\n",
    "            # Serialize and dump the output graph to the filesystem\n",
    "            with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "                f.write(output_graph_def.SerializeToString())\n",
    "            print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "            print(\"Wrote model to %s\" % output_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''graph = tf.get_default_graph()\n",
    "input_graph_def = graph.as_graph_def()\n",
    "node_names = [node.name for node in input_graph_def.node]\n",
    "print(node_names)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freeze_graph(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with tf.gfile.FastGFile(\"model/rnnmodel.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PREFIX = \"\"\n",
    "count =10\n",
    "with open ('./results1.txt', 'rt') as in_file:\n",
    "    for cnt,line in enumerate(in_file): \n",
    "        #print(line) # prints that line\n",
    "        tmp = line.split(\"(\")\n",
    "        #print(tmp)\n",
    "        if cnt >1 :\n",
    "            TEST_PREFIX += tmp[0] + ','\n",
    "        if cnt ==22 :\n",
    "            TEST_PREFIX += tmp[0]   \n",
    "        \n",
    "        if cnt == 22:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''absolute_model_folder = \"./model\"\n",
    "model_file_name = \"model_recipe\"\n",
    "input_checkpoint = absolute_model_folder + \"/\" + model_file_name'''\n",
    "#input_checkpoint = \"model/model_recipe.meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with tf.device(GPU_USE):\n",
    "    with tf.Session() as sess:\n",
    "        #saver.restore(sess, input_checkpoint)\n",
    "        #TEST_PREFIX = \"The \"\n",
    "        LEN_TEST_TEXT = 500\n",
    "        #TEST_PREFIX= \"eggs,bread,onion,butter,paprika\"\n",
    "        for i in range(len(TEST_PREFIX)):\n",
    "            out = net.run_step(embed_to_vocab(TEST_PREFIX[i], vocab), i == 0)\n",
    "            \n",
    "        print(\"Sentence:\")\n",
    "        gen_str = TEST_PREFIX\n",
    "        for i in range(LEN_TEST_TEXT):\n",
    "            # Sample character from the network according to the generated\n",
    "            # output probabilities\n",
    "            element = np.random.choice(range(len(vocab)), p=out)\n",
    "            gen_str += vocab[element]\n",
    "            out = net.run_step(embed_to_vocab(vocab[element], vocab), False)\n",
    "        print(gen_str)\n",
    "        f = open('saved/test_recipe.txt','w')\n",
    "        f.write(TEST_PREFIX + '\\n')\n",
    "        f.write(gen_str)\n",
    "        f.close()    \n",
    "    '''\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
